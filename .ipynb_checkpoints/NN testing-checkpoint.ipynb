{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\projects\\python projects\\venv\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in d:\\projects\\python projects\\venv\\lib\\site-packages (from nltk) (0.14.0)\n",
      "Requirement already satisfied: regex in d:\\projects\\python projects\\venv\\lib\\site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied: click in d:\\projects\\python projects\\venv\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in d:\\projects\\python projects\\venv\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\projects\\python projects\\venv\\lib\\site-packages (from tqdm->nltk) (3.2.1)\n",
      "Requirement already satisfied: colorama in d:\\projects\\python projects\\venv\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.4 in d:\\projects\\python projects\\venv\\lib\\site-packages (from importlib-resources->tqdm->nltk) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in d:\\projects\\python projects\\venv\\lib\\site-packages (from zipp>=0.4->importlib-resources->tqdm->nltk) (7.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp35-cp35m-win_amd64.whl (24.2 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "Requirement already satisfied: six>=1.5.0 in d:\\projects\\python projects\\venv\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in d:\\projects\\python projects\\venv\\lib\\site-packages (from gensim) (1.16.2)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp35-cp35m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\projects\\python projects\\venv\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: requests in d:\\projects\\python projects\\venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\projects\\python projects\\venv\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\projects\\python projects\\venv\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\python projects\\venv\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\projects\\python projects\\venv\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.26.9)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107097 sha256=c1d9188a78187da4da162e1b7bb059aa46a3e11b74b6be5bf761716d98fc67df\n",
      "  Stored in directory: c:\\users\\milan\\appdata\\local\\pip\\cache\\wheels\\d6\\b7\\9c\\9880636bd2f16fcca95037d4f4734dd0fcf1338d764e69d43c\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-3.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding, GRU, Bidirectional\n",
    "from keras.models import Model \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential, Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_CNN_BiGRU(filters = 100, kernel_size = 3, activation='relu', \n",
    "                   input_dim = None, output_dim=300, max_length = None, emb_matrix = None):\n",
    "  \n",
    "    # Channel 1D CNN\n",
    "    input1 = Input(shape=(max_length,))\n",
    "    embeddding1 = Embedding(input_dim=input_dim, \n",
    "                            output_dim=output_dim, \n",
    "                            input_length=max_length, \n",
    "                            input_shape=(max_length, ),\n",
    "                            # Assign the embedding weight with word2vec embedding marix\n",
    "                            weights = [emb_matrix],\n",
    "                            # Set the weight to be not trainable (static)\n",
    "                            trainable = False)(input1)\n",
    "    conv1 = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', \n",
    "                   kernel_constraint= MaxNorm( max_value=3, axis=[0,1]))(embeddding1)\n",
    "    pool1 = MaxPool1D(pool_size=2, strides=2)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    drop1 = Dropout(0.5)(flat1)\n",
    "    dense1 = Dense(10, activation='relu')(drop1)\n",
    "    drop1 = Dropout(0.5)(dense1)\n",
    "    out1 = Dense(1, activation='sigmoid')(drop1)\n",
    "    \n",
    "    # Channel BiGRU\n",
    "    input2 = Input(shape=(max_length,))\n",
    "    embeddding2 = Embedding(input_dim=input_dim, \n",
    "                            output_dim=output_dim, \n",
    "                            input_length=max_length, \n",
    "                            input_shape=(max_length, ),\n",
    "                            # Assign the embedding weight with word2vec embedding marix\n",
    "                            weights = [emb_matrix],\n",
    "                            # Set the weight to be not trainable (static)\n",
    "                            trainable = False,\n",
    "                            mask_zero=True)(input2)\n",
    "    gru2 = Bidirectional(GRU(64))(embeddding2)\n",
    "    drop2 = Dropout(0.5)(gru2)\n",
    "    out2 = Dense(1, activation='sigmoid')(drop2)\n",
    "    \n",
    "    # Merge\n",
    "    merged = concatenate([out1, out2])\n",
    "    \n",
    "    # Interpretation\n",
    "    outputs = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/train_preprocessed.csv')\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15485,), (15485,), (3872,), (3872,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stratify_y = data['class']\n",
    "train_x, test_x, train_y, test_y = train_test_split(data['text'],train_stratify_y,test_size=0.2,random_state=42,stratify=train_stratify_y)\n",
    "train_x.shape,train_y.shape,test_x.shape,test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "train_x = train_x.apply(tokenizer.tokenize)\n",
    "test_x = test_x.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n",
      "Time to train the model: 2.44 mins\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from time import time \n",
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "#BUILD_VOCAB()\n",
    "t = time()\n",
    "w2v_model.build_vocab(train_x, progress_per=1000)\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "#TRAIN()\n",
    "w2v_model.train(train_x, total_examples=w2v_model.corpus_count, epochs=1000, report_delay=1)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15485,), (15485,), (3872,), (3872,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(data['text'],train_stratify_y,test_size=0.2,random_state=42,stratify=train_stratify_y)\n",
    "train_x.shape,train_y.shape,test_x.shape,test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 11000\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), lowercase=True, max_features=11000)\n",
    "matrix = vectorizer.fit_transform(np.array(train_x))\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\python projects\\venv\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "d:\\projects\\python projects\\venv\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape for training set :  (15485, 300) \n",
      "shape for test set :  (3872, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, train_x)])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, test_x)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "print ('shape for training set : ',train_vecs_w2v.shape,\n",
    "      '\\nshape for test set : ', test_vecs_w2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 38,657\n",
      "Trainable params: 38,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=300))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15485 samples, validate on 3872 samples\n",
      "Epoch 1/20\n",
      "15485/15485 [==============================] - 1s 73us/step - loss: 0.0388 - accuracy: 0.0229 - val_loss: -0.6652 - val_accuracy: 0.0227\n",
      "Epoch 2/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -1.4286 - accuracy: 0.0226 - val_loss: -2.2192 - val_accuracy: 0.0227\n",
      "Epoch 3/20\n",
      "15485/15485 [==============================] - 0s 28us/step - loss: -3.0399 - accuracy: 0.0226 - val_loss: -3.8818 - val_accuracy: 0.0227\n",
      "Epoch 4/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -4.7528 - accuracy: 0.0226 - val_loss: -5.6453 - val_accuracy: 0.0227\n",
      "Epoch 5/20\n",
      "15485/15485 [==============================] - 0s 26us/step - loss: -6.5675 - accuracy: 0.0226 - val_loss: -7.5099 - val_accuracy: 0.0227\n",
      "Epoch 6/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -8.4852 - accuracy: 0.0226 - val_loss: -9.4812 - val_accuracy: 0.0227\n",
      "Epoch 7/20\n",
      "15485/15485 [==============================] - 0s 26us/step - loss: -10.5135 - accuracy: 0.0226 - val_loss: -11.5664 - val_accuracy: 0.0227\n",
      "Epoch 8/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -12.6539 - accuracy: 0.0226 - val_loss: -13.7617 - val_accuracy: 0.0227\n",
      "Epoch 9/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -14.9040 - accuracy: 0.0226 - val_loss: -16.0634 - val_accuracy: 0.0227\n",
      "Epoch 10/20\n",
      "15485/15485 [==============================] - 0s 28us/step - loss: -17.2543 - accuracy: 0.0226 - val_loss: -18.4588 - val_accuracy: 0.0227\n",
      "Epoch 11/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -19.6985 - accuracy: 0.0226 - val_loss: -20.9492 - val_accuracy: 0.0227\n",
      "Epoch 12/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -22.2283 - accuracy: 0.0226 - val_loss: -23.5176 - val_accuracy: 0.0227\n",
      "Epoch 13/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -24.8343 - accuracy: 0.0226 - val_loss: -26.1579 - val_accuracy: 0.0227\n",
      "Epoch 14/20\n",
      "15485/15485 [==============================] - 0s 26us/step - loss: -27.5094 - accuracy: 0.0226 - val_loss: -28.8669 - val_accuracy: 0.0227\n",
      "Epoch 15/20\n",
      "15485/15485 [==============================] - 0s 26us/step - loss: -30.2553 - accuracy: 0.0226 - val_loss: -31.6456 - val_accuracy: 0.0227\n",
      "Epoch 16/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -33.0645 - accuracy: 0.0226 - val_loss: -34.4827 - val_accuracy: 0.0227\n",
      "Epoch 17/20\n",
      "15485/15485 [==============================] - 0s 26us/step - loss: -35.9305 - accuracy: 0.0226 - val_loss: -37.3771 - val_accuracy: 0.0227\n",
      "Epoch 18/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -38.8515 - accuracy: 0.0226 - val_loss: -40.3191 - val_accuracy: 0.0227\n",
      "Epoch 19/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -41.8177 - accuracy: 0.0226 - val_loss: -43.3096 - val_accuracy: 0.0227\n",
      "Epoch 20/20\n",
      "15485/15485 [==============================] - 0s 27us/step - loss: -44.8359 - accuracy: 0.0226 - val_loss: -46.3506 - val_accuracy: 0.0227\n",
      "Training Accuracy: 0.0226\n",
      "Testing Accuracy:  0.0227\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_vecs_w2v, train_y, epochs=20, batch_size=50,\n",
    "                   validation_data=(test_vecs_w2v, test_y))\n",
    "loss, accuracy = model.evaluate(train_vecs_w2v, train_y, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_vecs_w2v, test_y, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
